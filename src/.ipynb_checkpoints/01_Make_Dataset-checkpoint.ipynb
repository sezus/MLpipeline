{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Collect_Data_From_CSV\n",
    "import Create_Labels\n",
    "import Clean_Normalize\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['log', 'log', 'log', 'log']\n",
      "['T03.xls', 'T03.xls', 'T03.xls', 'T03.xls']\n",
      "['Gen.Operation Data', 'Gearbox Data', 'Main Bearing Data', 'Pitch System Data']\n",
      "[[0, 11], [0, 11], [0, 11], [0, 11]]\n"
     ]
    }
   ],
   "source": [
    "# Function to load yaml configuration file\n",
    "def load_config(config_name):\n",
    "    with open(os.path.join(CONFIG_PATH,  config_name), 'r', encoding='utf8') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config\n",
    "\n",
    "#yaml file Load path:\n",
    "CONFIG_PATH = \".\"\n",
    "\n",
    "#Load project config from yaml file:\n",
    "config = load_config('importcsv.yaml')\n",
    "\n",
    "#Dataset Tags\n",
    "Upload_tag=config[\"Upload_list_all\"][\"Upload_Tags\"]\n",
    "tag=config[\"Upload_list_all\"][Upload_tag[1]]\n",
    "\n",
    "#Raw Data path\n",
    "raw_path=config[\"Upload_list_all\"]['Parent_path']+\"data/01_raw/\" \n",
    "upload_path=raw_path+\"uploaded/\" \n",
    "\n",
    "#Intermediate path\n",
    "intermediate_path=config[\"Upload_list_all\"]['Parent_path']+\"/data/02_intermediate/\" \n",
    "parent_path=config[\"Upload_list_all\"]['Parent_path']\n",
    "\n",
    "#####01_Load CSV#####\n",
    "#01_1.Load Logs:\n",
    "Logs=Collect_Data_From_CSV.upload_multiple_sheets(parent_path,tag['Path_list'],tag['File_list'],tag['Sheet_list'],tag['Range_list'],tag['refcolumn'])\n",
    "#01_2.load faults:\n",
    "Faults=Collect_Data_From_CSV.upload_single_sheet(parent_path,tag['Fault_path'],tag['Fault_file'],tag['Fault_sheet'],skiprows=tag['Fault_range'])    \n",
    "\n",
    "#Save Uploaded Dataframes as Logs and Faults from Raw data:\n",
    "with open(upload_path +'Logs'+ Upload_tag[1], 'wb') as config_file:\n",
    "    pickle.dump(Logs, config_file)\n",
    "with open(upload_path +'Faults'+ Upload_tag[1], 'wb') as config_file:\n",
    "    pickle.dump(Faults, config_file)\n",
    "    \n",
    "\n",
    "#####02_Clean & Normalize Logs(Features) data#####\n",
    "#02_1.rename columns,normalize,fill empty cells, burda normalize ve fill empty cells adımları ayrılabilir.\n",
    "Logs,Logs_not_normal=Clean_Normalize.anomaly_normalize(Logs,tag['renamecolumns'],tag['feature_fill_list'],tag['feature_fill_list2'],tag['normalize_feature_list'])\n",
    "\n",
    "#02_2.Combine Logs and Fault(Target) dataframes according to Period,Start Time,End Time\n",
    "Logs_Faults=Clean_Normalize.combine_df_Period(Logs,['Period','Events'],Faults,['Start Time', 'End Time','Event ID'])\n",
    "\n",
    "\n",
    "#02_3 Save Normalize Logs & Not Normalize Logs & Combined Dataset Logs+Faults:\n",
    "with open(intermediate_path +'Logs_normal_fill'+Upload_tag[1], 'wb') as config_file:\n",
    "    pickle.dump(Logs, config_file)\n",
    "with open(intermediate_path +'Logs_not_normal_fill'+Upload_tag[1], 'wb') as config_file:\n",
    "    pickle.dump(Logs_not_normal, config_file)\n",
    "with open(intermediate_path +'Logs_Faults'+Upload_tag[1], 'wb') as config_file:\n",
    "    pickle.dump(Logs_Faults, config_file)\n",
    "\n",
    "#####03_Create Labels#####\n",
    "#03_1 Event referans alarak once binary sonra multiclass sonra da onehot labellar oluştur.\n",
    "#Yeni Labellar oluşturmadan önceki kolon sayısı before_size_df\n",
    "default_size=Logs_Faults.shape[1]\n",
    "#Labelları oluştur.\n",
    "Logs_Faults=Create_Labels.create_binary_multiclass_onehot_labels(Logs_Faults,'Events')\n",
    "\n",
    "#03_2 Main Category ve Category gibi ekstra Eventleri sınıflandırabilecegimiz bilgileri \n",
    "#dışardan bir excelden almamız gerekebilir.\n",
    "\n",
    "#print(Logs_Faults[Logs_Faults.columns[Logs_Faults.shape[1]]])\n",
    "FaultAllCategoryEvents=Create_Labels.fault(Faults,raw_path+'scada/Umut_SCADA.xls')\n",
    "\n",
    "#03_3 Yeni gelen Categoryleri Oluşan Eventlerle birlestirip\n",
    "#Eventlerin karşılık geldiği Componentlere ait binary & multiclass & onehot labellar oluşturma işlemi:\n",
    "Logs_Faults=Create_Labels.create_component_labels(Logs_Faults,'Events',FaultAllCategoryEvents,'Event ID','Main Category')\n",
    "\n",
    "#03_4 Son olarak Oluşturulan Labelları farklı bir Dataframe olarak ayır: \"Label\"  \n",
    "Labelcolumns=[0,]\n",
    "Labels=Logs_Faults[Logs_Faults.columns[range(default_size,Logs_Faults.shape[1])]]\n",
    "Labels['Period']=Logs_Faults['Period']\n",
    "\n",
    "#03_5 Featureları da ayrı bir Dataframe olarak tut: \"Features\"\n",
    "Features=Logs_Faults.drop(Logs_Faults.columns[range(default_size,Logs_Faults.shape[1])], axis='columns')\n",
    "\n",
    "#03_6 Save Clean Dataset as Label & Features:\n",
    "with open(intermediate_path + 'Labels'+Upload_tag[1], 'wb') as config_file:\n",
    "    pickle.dump(Label, config_file)\n",
    "with open(intermediate_path + 'Features'+Upload_tag[1], 'wb') as config_file:\n",
    "    pickle.dump(Features, config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
