{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('..') \n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from src.data.Collect_Data_From_CSV import upload_multiple_sheets,upload_single_sheet\n",
    "from src.intermediate.Clean_Normalize import anomaly_normalize,combine_df_Period\n",
    "from src.intermediate.Create_Labels import create_binary_multiclass_onehot_labels,fault,create_component_labels\n",
    "from src.utils.utils_p import YamlParser,save_dataset_pickle\n",
    "from src.intermediate.Data_Preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD CONFIG PARSE PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####LOAD CONFIG PARSE PATHS#####\n",
    "#yaml file Load path:\n",
    "CONFIG_PATH = \"D:/Users/sezgis/Desktop/Projects/Git/SmartPDM/Diagnostic_Pitch_System_Final/src\"\n",
    "\n",
    "#Load project config from yaml file:\n",
    "yaml_parser = YamlParser(CONFIG_PATH,'importcsv.yaml')\n",
    "config = yaml_parser.get_yaml_file()\n",
    "\n",
    "#Dataset Tags:\n",
    "Upload_tag=config[\"Upload_list_all\"][\"Upload_Tags\"]\n",
    "\n",
    "#Select Tag to Upload\n",
    "TagName=Upload_tag[0]\n",
    "\n",
    "#Dataset Information:\n",
    "tag=config[\"Upload_list_all\"][TagName]\n",
    "\n",
    "#Raw Data Path\n",
    "raw_path=config[\"Upload_list_all\"]['Parent_path']+\"data/01_raw/\" \n",
    "\n",
    "#Intermediate Path\n",
    "intermediate_path=config[\"Upload_list_all\"]['Parent_path']+\"/data/02_intermediate/\" \n",
    "\n",
    "#Parent Project Path\n",
    "parent_path=config[\"Upload_list_all\"]['Parent_path'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['log', 'log', 'log', 'log']\n",
      "['T02.xls', 'T02.xls', 'T02.xls', 'T02.xls']\n",
      "['Gen.Operation Data', 'Gearbox Data', 'Main Bearing Data', 'Pitch System Data']\n",
      "[[0, 11], [0, 11], [0, 11], [0, 11]]\n"
     ]
    }
   ],
   "source": [
    "#####01_Load CSV#####\n",
    "#01_1.Load Logs:\n",
    "Logs=upload_multiple_sheets(parent_path,tag['Path_list'],tag['File_list'],tag['Sheet_list'],tag['Range_list'],tag['refcolumn'])\n",
    "#01_2.load faults:\n",
    "Faults=upload_single_sheet(parent_path,tag['Fault_path'],tag['Fault_file'],tag['Fault_sheet'],skiprows=tag['Fault_range'])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Uploaded Dataframes as Logs and Faults from Raw data:\n",
    "save_dataset_pickle(raw_path,'Logs'+TagName,Logs)\n",
    "save_dataset_pickle(raw_path,'Faults'+TagName,Faults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean & Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####02_Clean & Normalize Logs(Features) data#####\n",
    "#02_1.rename columns,normalize,fill empty cells, burda normalize ve fill empty cells adımları ayrılabilir.\n",
    "Logs,Logs_not_normal=anomaly_normalize(Logs,tag['renamecolumns'],tag['feature_fill_list'],tag['feature_fill_list2'],tag['normalize_feature_list'])\n",
    "\n",
    "#02_2.Combine Logs and Fault(Target) dataframes according to Period,Start Time,End Time\n",
    "Logs_Faults=combine_df_Period(Logs,['Period','Events'],Faults,['Start Time', 'End Time','Event ID'])\n",
    "\n",
    "\n",
    "#02_3 Save Normalize Logs & Not Normalize Logs & Combined Dataset Logs+Faults:\n",
    "save_dataset_pickle(intermediate_path,'Logs_normal_fill' + TagName,Logs)\n",
    "save_dataset_pickle(intermediate_path,'Logs_not_normal_fill'+ TagName,Logs_not_normal)\n",
    "save_dataset_pickle(intermediate_path,'Logs_Faults' + TagName,Logs_Faults)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Users/sezgis/Desktop/Projects/Git/SmartPDM/Diagnostic_Pitch_System_Final/data/01_raw/scada/Umut_SCADA.xls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\sezgis\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#####03_Create Labels#####\n",
    "#03_1 Event referans alarak once binary sonra multiclass sonra da onehot labellar oluştur.\n",
    "#Yeni Labellar oluşturmadan önceki kolon sayısı before_size_df\n",
    "default_size=Logs_Faults.shape[1]\n",
    "#Labelları oluştur.\n",
    "Logs_Faults=create_binary_multiclass_onehot_labels(Logs_Faults,'Events')\n",
    "\n",
    "#03_2 Main Category ve Category gibi ekstra Eventleri sınıflandırabilecegimiz bilgileri \n",
    "#dışardan bir excelden almamız gerekebilir.\n",
    "\n",
    "#print(Logs_Faults[Logs_Faults.columns[Logs_Faults.shape[1]]])\n",
    "FaultAllCategoryEvents=fault(Faults,raw_path+'scada/Umut_SCADA.xls')\n",
    "\n",
    "#03_3 Yeni gelen Categoryleri Oluşan Eventlerle birlestirip\n",
    "#Eventlerin karşılık geldiği Componentlere ait binary & multiclass & onehot labellar oluşturma işlemi:\n",
    "Logs_Faults=create_component_labels(Logs_Faults,'Events',FaultAllCategoryEvents,'Event ID','Main Category')\n",
    "\n",
    "#03_4 Son olarak Oluşturulan Labelları farklı bir Dataframe olarak ayır: \"Label\"  \n",
    "#Labelcolumns=[0,]\n",
    "Labels=Logs_Faults[Logs_Faults.columns[range(default_size,Logs_Faults.shape[1])]]\n",
    "Labels['Period']=Logs_Faults['Period']\n",
    "\n",
    "#03_5 Featureları da ayrı bir Dataframe olarak tut: \"Features\"\n",
    "Features=Logs_Faults.drop(Logs_Faults.columns[range(default_size,Logs_Faults.shape[1])], axis='columns')\n",
    "\n",
    "#03_6 Save Clean Dataset as Label & Features:\n",
    "save_dataset_pickle(intermediate_path,'Labels' + TagName,Labels)\n",
    "save_dataset_pickle(intermediate_path,'Features' + TagName,Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
